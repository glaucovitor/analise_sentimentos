{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCC Augusto e Ícaro\n",
    "## Modelo de automatização das Heurísticas de Nielsen para comentários em reviews de Apps\n",
    "\n",
    "* Versão 0.1.0\n",
    "* Bibliotecas utilizadas: pandas, numpy, texthero, ntlk e corpus do ntlk em português\n",
    "* Dataset utilizado: dataset_v4.csv\n",
    "* Data: 22/07/2020\n",
    "\n",
    "### Objetivos principais:\n",
    "\n",
    "* Detalhar e implementar pipeline de pre-processamento\n",
    "\n",
    "* Implementar classificador baseado em ocorrência, sem foco na acurácia do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (1.0.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: numpy in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (1.18.5)\n",
      "Requirement already satisfied: texthero in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (1.0.9)\n",
      "Requirement already satisfied: tqdm>=4.3 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (4.46.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (0.23.1)\n",
      "Requirement already satisfied: pandas>=1.0.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (1.0.5)\n",
      "Requirement already satisfied: wordcloud>=1.5.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (1.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (3.2.2)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (2.3.2)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (1.1.1)\n",
      "Requirement already satisfied: nltk>=3.3 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (1.18.5)\n",
      "Requirement already satisfied: gensim>=3.6.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (3.8.3)\n",
      "Requirement already satisfied: plotly>=4.2.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from texthero) (4.9.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (0.15.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas>=1.0.2->texthero) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas>=1.0.2->texthero) (2.8.1)\n",
      "Requirement already satisfied: pillow in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (1.2.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (47.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (0.7.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from spacy>=2.2.2->texthero) (1.0.2)\n",
      "Requirement already satisfied: click in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from nltk>=3.3->texthero) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from nltk>=3.3->texthero) (2020.6.8)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from gensim>=3.6.0->texthero) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from gensim>=3.6.0->texthero) (1.15.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (1.25.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (1.6.1)\n",
      "Requirement already satisfied: boto in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim>=3.6.0->texthero) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim>=3.6.0->texthero) (1.14.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.1.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.23 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (1.17.23)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.23->boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (0.15.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: regex in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from nltk) (4.46.1)\n",
      "Requirement already satisfied: click in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/icaro/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from nltk) (0.15.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install texthero \n",
    "!{sys.executable} -m pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import texthero as hero\n",
    "import nltk\n",
    "import numpy as np\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa versão iremos testar apenas com textos em português, sem adicionar os pesos de sentimentos e heurísticas de Nielsen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos treinar o modelo para classificar apenas em usabilidade ou não.\n",
    "Para isso, iremos utilizar apenas as colunas `is_usability` e `is_classified` do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3eb8ab658d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_usability'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_classified'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reviews_v4.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "cols = ['ID', 'Text', 'is_usability', 'is_classified']\n",
    "df = pd.read_csv(\"reviews_v4.csv\", index_col=False, usecols=cols)\n",
    "df = df.rename(columns={'Text': 'text'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    755\n",
       "True     175\n",
       "Name: is_usability, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_usability'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    548\n",
       "True     381\n",
       "Name: is_classified, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_classified'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpando dados\n",
    "\n",
    "O pipeline padrão remove dígitos, pontuação, remove diacritics, stopwords em inglês e whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      negocio de reconhecimento facialnao funciona n...\n",
       "1      um lixo tentativas de econhecimento facial sem...\n",
       "2      horrivel pior que fbi se fosse pra receberem a...\n",
       "3           meio dificil fazer sem oculos mais deu certo\n",
       "4         nao serve pra nada nao da para acessar pessimo\n",
       "                             ...                        \n",
       "925    como eu faco pra entrar na minha conta enem pe...\n",
       "926    sim facilita muito em varios servicos e mais i...\n",
       "927    nao consigo acessar meu auxilio como faco pra ...\n",
       "928                                  seguro hiper seguro\n",
       "929    otimo porem e bom implementar novas funcionali...\n",
       "Name: text, Length: 930, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = hero.preprocessing.get_default_pipeline()\n",
    "selected_functions_indexes = [0, 1, 2, 5, 6]\n",
    "pipeline = [pipeline[i] for i in selected_functions_indexes]\n",
    "df['text'] = hero.preprocessing.clean(df['text'])\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Removendo stopwords em português com o corpus em portugês do NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negocio  reconhecimento facialnao funciona nao  pra ficar dia todo nisso nao arrumem   favor'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "df['text'] = hero.remove_stopwords(df['text'], stopwords=pt_stopwords)\n",
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negoci reconhec facialna funcion nao pra fic dia tod niss nao arrum favor'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = hero.stem(df['text'], language='portuguese')\n",
    "df['text']\n",
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>is_usability</th>\n",
       "      <th>is_classified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gp:AOqpTOEAoU7yqbku6p11S8xAmTVWRkt0qC0lG4I-XH-...</td>\n",
       "      <td>[negoci, reconhec, facialna, funcion, nao, pra...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gp:AOqpTOGoBVvBdtutEKaPnD7--OBC8nDGbSuRniM3Vbm...</td>\n",
       "      <td>[lix, tentat, econhec, facial, exit, dur, depe...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gp:AOqpTOFUV6HfFseu9It6e2HTJy3CWzCuNgqCyelLRka...</td>\n",
       "      <td>[horrivel, pior, fbi, pra, receb, algo, gent, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gp:AOqpTOG3mZqb8oLGMk-njqh5ycoCJ0EMNIVEghrRUzg...</td>\n",
       "      <td>[mei, dificil, faz, ocul, deu, cert]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gp:AOqpTOE-kXsLJBlMyY8W7NWsdk_XrfVsYpOZSATrW5v...</td>\n",
       "      <td>[nao, serv, pra, nad, nao, acess, pessim]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ID  \\\n",
       "0  gp:AOqpTOEAoU7yqbku6p11S8xAmTVWRkt0qC0lG4I-XH-...   \n",
       "1  gp:AOqpTOGoBVvBdtutEKaPnD7--OBC8nDGbSuRniM3Vbm...   \n",
       "2  gp:AOqpTOFUV6HfFseu9It6e2HTJy3CWzCuNgqCyelLRka...   \n",
       "3  gp:AOqpTOG3mZqb8oLGMk-njqh5ycoCJ0EMNIVEghrRUzg...   \n",
       "4  gp:AOqpTOE-kXsLJBlMyY8W7NWsdk_XrfVsYpOZSATrW5v...   \n",
       "\n",
       "                                                text  is_usability  \\\n",
       "0  [negoci, reconhec, facialna, funcion, nao, pra...          True   \n",
       "1  [lix, tentat, econhec, facial, exit, dur, depe...         False   \n",
       "2  [horrivel, pior, fbi, pra, receb, algo, gent, ...          True   \n",
       "3               [mei, dificil, faz, ocul, deu, cert]          True   \n",
       "4          [nao, serv, pra, nad, nao, acess, pessim]          True   \n",
       "\n",
       "  is_classified  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = hero.tokenize(df['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class_name'] = df.apply(lambda row: 'usability' if row['is_usability'] else 'not_usability', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'gp:AOqpTOEAoU7yqbku6p11S8xAmTVWRkt0qC0lG4I-XH-aKqUYoBvlXRVBPmOCEfu4c8d3x95IvEF06yqPaEDEwQ',\n",
       " 'text': ['negoci',\n",
       "  'reconhec',\n",
       "  'facialna',\n",
       "  'funcion',\n",
       "  'nao',\n",
       "  'pra',\n",
       "  'fic',\n",
       "  'dia',\n",
       "  'tod',\n",
       "  'niss',\n",
       "  'nao',\n",
       "  'arrum',\n",
       "  'favor'],\n",
       " 'is_classified': True,\n",
       " 'class_name': 'usability'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('is_usability', 1)\n",
    "df = df.to_dict('records')\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "Nessa parte, iremos treinar e utilizar o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para aprendizado dos dados: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(training_data):\n",
    "    corpus_words = {}\n",
    "    for data in training_data: \n",
    "        class_name = data['class_name']\n",
    "        frase = data['text']\n",
    "        if class_name not in list(corpus_words.keys()):\n",
    "            corpus_words[class_name] = {}\n",
    "        for word in frase:\n",
    "            if word not in list(corpus_words[class_name].keys()):\n",
    "                corpus_words[class_name][word] = 1\n",
    "            else:\n",
    "                corpus_words[class_name][word] += 1\n",
    "    return corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_score(corpus_words):\n",
    "    for word in list(corpus_words['usability'].keys()):\n",
    "        if corpus_words['usability'][word] < 10:\n",
    "            del corpus_words['usability'][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para calcular scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificate(corpus, sentence):\n",
    "    def calculate_class_score(corpus_words, sentence, class_name):\n",
    "        score = 0 \n",
    "        for word in sentence:\n",
    "            if word in corpus_words[class_name]:\n",
    "                score += corpus_words[class_name][word]\n",
    "        return score\n",
    "    classifications = []\n",
    "    for class_name in corpus.keys():\n",
    "        classifications.append({'class_name': class_name, 'score': calculate_class_score(corpus, sentence, class_name)})    \n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para normalizar os scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(classification):\n",
    "    total_score = sum(score['score'] for score in classification['scores'])\n",
    "    if total_score != 0:\n",
    "        for score in classification['scores']:\n",
    "            score['score'] = score['score']/total_score\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função principal\n",
    "\n",
    "* Divide o dataset em classificados e não classificados.\n",
    "* Divide os classificados para teste e treino: 20% teste e 80% treino.\n",
    "* Calcula em 50 interações a média da acurácia do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "usability_corpus = {\n",
    "     'form': 100,\n",
    "     'celul': 100,\n",
    "     'vez': 110,\n",
    "     'consegu': 120,\n",
    "     'email': 120,\n",
    "     'pod': 130,\n",
    "     'ajud': 130,\n",
    "     'opca': 130,\n",
    "     'ped': 130,\n",
    "     'entrar': 140,\n",
    "     'fiz': 140,\n",
    "     'fot': 140,\n",
    "     'ser': 150,\n",
    "     'aparec': 150,\n",
    "     'prov': 150,\n",
    "     'fac': 150,\n",
    "     'diz': 160,\n",
    "     'temp': 170,\n",
    "     'tod': 170,\n",
    "     'cnh': 170,\n",
    "     'nad': 180,\n",
    "     'vid': 190,\n",
    "     'fic': 190,\n",
    "     'cont': 200,\n",
    "     'cpf': 210,\n",
    "     'recuper': 220,\n",
    "     'precis': 250,\n",
    "     'acess': 260,\n",
    "     'reconhec': 270,\n",
    "     'erro': 280,\n",
    "     'aplic': 290,\n",
    "     'funcion': 290,\n",
    "     'facial': 300,\n",
    "     'app': 350,\n",
    "     'tent': 370,\n",
    "     'cadastr': 410,\n",
    "     'consig': 430,\n",
    "     'faz': 560,\n",
    "     'senh': 570,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Accuracy 0.10174418604651163\n",
      "-----------------\n",
      "Final accuracy 0.10174418604651163\n",
      "Right answers usa: 149  qtd 158\n",
      "Right answers no_usa: 66 qtd 186\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "classified_df = []\n",
    "unclassified_df = []\n",
    "\n",
    "for data in df:\n",
    "    classified_df.append(data) if data['is_classified'] else unclassified_df.append(data)\n",
    "\n",
    "accuracies = []\n",
    "iterations = 1\n",
    "for i in range(iterations):\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    random.shuffle(classified_df)\n",
    "    train_dataset, test_dataset = np.split(classified_df, [int(len(classified_df)*0.1)])\n",
    "        \n",
    "    usa = [data for data in test_dataset if data['class_name']=='usability']\n",
    "    not_usa = [data for data in test_dataset if data['class_name']=='not_usability']\n",
    "\n",
    "#     corpus = learning(train_dataset)\n",
    "    corpus['usability'] = tmp_corpus\n",
    "    classifications = [{'test_data': test_data, 'scores': classificate(corpus, test_data['text'])} for test_data in test_dataset]\n",
    "    classifications = [normalize_scores(classification) for classification in classifications]\n",
    "    \n",
    "    right_answers_usa = 0\n",
    "    right_answers_no_usa = 0\n",
    "    for classification in classifications:\n",
    "        higher_score = max(classification['scores'], key=lambda x:x['score'])\n",
    "        if classification['test_data']['class_name'] == higher_score['class_name']:\n",
    "            if classification['test_data']['class_name'] == 'usability':\n",
    "                right_answers_usa+=1\n",
    "            else: \n",
    "                right_answers_no_usa+=1\n",
    "        else: \n",
    "            print(classification)\n",
    "    accuracy = right_answers/len(classifications)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Iteration {i} - Accuracy {accuracy}')\n",
    "\n",
    "print('-----------------')\n",
    "print(f'Final accuracy {sum(accuracies)/len(accuracies)}')\n",
    "print(f'Right answers usa: {right_answers_usa}  qtd {len(usa)}')\n",
    "print(f'Right answers no_usa: {right_answers_no_usa} qtd {len(not_usa)}')\n",
    "print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'form': 100, 'celul': 100, 'vez': 110, 'consegu': 120, 'email': 120, 'pod': 130, 'ajud': 130, 'opca': 130, 'ped': 130, 'entrar': 140, 'fiz': 140, 'fot': 140, 'ser': 150, 'aparec': 150, 'prov': 150, 'fac': 150, 'diz': 160, 'temp': 170, 'tod': 170, 'cnh': 170, 'nad': 180, 'vid': 190, 'fic': 190, 'cont': 200, 'cpf': 210, 'recuper': 220, 'precis': 250, 'acess': 260, 'reconhec': 270, 'erro': 280, 'aplic': 290, 'funcion': 290, 'facial': 300, 'app': 350, 'tent': 370, 'cadastr': 410, 'consig': 430, 'faz': 560, 'senh': 570}\n"
     ]
    }
   ],
   "source": [
    "print(corpus['usability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horrivel': 7, 'app': 19, 'perd': 5, 'temp': 7, 'nao': 95, 'merec': 2, 'estrel': 5, 'dev': 5, 'ter': 6, 'pratic': 2, 'clarez': 1, 'funcion': 19, 'ped': 6, 'senh': 17, 'tempor': 1, 'envi': 1, 'pessim': 29, 'experienc': 5, 'nov': 3, 'plataform': 2, 'consig': 20, 'recuper': 7, 'acess': 15, 'nenhum': 4, 'opco': 2, 'fornec': 1, 'quer': 1, 'volt': 3, 'transtorn': 1, 'ser': 7, 'facil': 4, 'execut': 1, 'tend': 2, 'vist': 1, 'muit': 2, 'pesso': 4, 'nest': 2, 'moment': 1, 'dificuldad': 2, 'precis': 3, 'estar': 4, 'present': 1, 'virtual': 1, 'cad': 2, 'dia': 2, 'dificil': 4, 'lamentavel': 1, 'prov': 5, 'vid': 8, 'cam': 2, 'centraliz': 1, 'ate': 2, 'pra': 17, 'fech': 1, 'olhos': 1, 'hack': 1, 'desenvolv': 1, 'algo': 2, 'tao': 1, 'amador': 1, 'aplic': 19, 'nad': 6, 'nel': 2, 'faz': 25, 'cadastr': 23, 'cpf': 9, 'ja': 15, 'aparec': 2, 'telefon': 3, 'email': 2, 'sao': 1, 'porfavor': 1, 'soluca': 1, 'algu': 3, 'utiliz': 1, 'coloc': 9, 'pod': 7, 'coment': 1, 'referenc': 1, 'ordem': 1, 'mundial': 1, 'chip': 1, 'gad': 1, 'estud': 2, 'sobr': 1, 'asunt': 1, 'ment': 1, 'aprofund': 1, 'consegu': 10, 'instal': 1, 'bom': 11, 'pedrer': 1, 'construca': 1, 'civil': 1, 'parabens': 4, 'perfeit': 2, 'maravilh': 1, 'ruim': 13, 'reconhec': 15, 'facial': 20, 'chat': 2, 'assunt': 1, 'interess': 1, 'por': 2, 'relaca': 1, 'curs': 2, 'deix': 2, 'desej': 1, 'tamb': 5, 'seman': 1, 'tent': 10, 'inscrev': 1, 'profoc': 1, 'import': 1, 'mim': 2, 'erro': 7, 'apnic': 1, 'dad': 4, 'porqu': 5, 'sempr': 5, 'pecim': 1, 'orient': 1, 'digit': 2, 'dao': 1, 'nom': 1, 'mae': 1, 'dat': 3, 'nasciment': 1, 'corret': 2, 'conform': 1, 'document': 3, 'orrivel': 2, 'tod': 7, 'afff': 2, 'complet': 1, 'lix': 9, 'perc': 1, 'pur': 1, 'enganaca': 1, 'pres': 1, 'band': 1, 'impossivel': 3, 'terrivel': 1, 'diz': 3, 'possu': 1, 'biometr': 6, 'bas': 1, 'mei': 1, 'comsig': 1, 'cri': 3, 'cont': 9, 'acho': 1, 'vou': 3, 'comsequ': 1, 'fal': 2, 'casatr': 1, 'fac': 8, 'mostr': 1, 'errad': 2, 'numer': 5, 'cel': 3, 'complic': 2, 'trabalh': 3, 'brasileir': 1, 'henriqu': 1, 'bell': 1, 'atend': 1, 'ae': 1, 'esper': 1, 'le': 1, 'codig': 2, 'qrcod': 2, 'avanc': 2, 'recuperaca': 4, 'drog': 1, 'coloqu': 1, 'negat': 1, 'valid': 2, 'hor': 2, 'segu': 1, 'aind': 3, 'cert': 3, 'otim': 5, 'dem': 1, 'ganh': 1, 'viu': 1, 'trav': 2, 'exist': 3, 'nunc': 6, 'feit': 3, 'conseg': 2, 'redefin': 2, 'deus': 1, 'usar': 1, 'cnh': 7, 'ultim': 1, 'atualizaca': 1, 'inutiliz': 1, 'validaca': 5, 'sequenc': 1, 'par': 2, 'pront': 2, 'hav': 2, 'agor': 2, 'morr': 1, 'lent': 1, 'simples': 2, 'ache': 1, 'pouc': 1, 'idad': 1, 'naveg': 4, 'ta': 2, 'seri': 1, 'problem': 3, 'carteir': 4, 'habilitaco': 1, 'gent': 2, 'process': 2, 'fot': 5, 'rap': 2, 'agent': 1, 'ler': 4, 'direit': 3, 'vai': 5, 'finaliz': 1, 'voc': 2, 'fic': 4, 'igual': 1, 'retard': 1, 'mov': 1, 'rost': 2, 'encerr': 1, 'excelent': 2, 'comig': 1, 'bug': 1, 'identificaca': 2, 'quatr': 1, 'tentat': 1, 'exit': 1, 'esta': 1, 'fiz': 2, 'opte': 1, 'leitur': 4, 'const': 2, 'oqu': 1, 'titul': 1, 'entend': 2, 'ajud': 4, 'boa': 2, 'noit': 1, 'habilitaca': 3, 'digital': 2, 'segund': 2, 'tdo': 1, 'vcs': 1, 'lig': 1, 'funca': 1, 'etap': 2, 'emissa': 2, 'form': 4, 'algum': 2, 'login': 1, 'urgent': 1, 'work': 1, 'duv': 2, 'pior': 2, 'irracion': 1, 'alem': 1, 'tud': 3, 'descriminatori': 1, 'exig': 1, 'tir': 3, 'ocul': 3, 'angul': 1, 'necessari': 1, 'tel': 1, 'comand': 1, 'usa': 1, 'gost': 4, 'atecnolog': 1, 'aprxim': 1, 'gov': 6, 'populaca': 1, 'promess': 1, 'burrocrac': 1, 'restant': 1, 'dinheir': 2, 'contribuint': 2, 'jog': 2, 'bem': 2, 'use': 3, 'la': 1, 'inform': 3, 'inic': 2, 'mail': 1, 'vi': 2, 'vergonh': 1, 'sequ': 1, 'bota': 1, 'ir': 1, 'facilit': 1, 'troc': 1, 'burocrac': 1, 'propri': 1, 'sit': 2, 'apen': 1, 'possivel': 2, 'autentificaca': 2, 'usand': 2, 'ridicul': 1, 'apps': 1, 'ruins': 1, 'super': 1, 'amo': 1, 'baix': 2, 'observ': 1, 'var': 2, 'vez': 3, 'entra': 1, 'usuari': 1, 'inval': 1, 'entrar': 3, 'funcional': 1, 'resolv': 2, 'leitor': 1, 'ves': 1, 'consegui': 1, 'assim': 1, 'sei': 1, 'estagiari': 1, 'program': 1, 'der': 1, 'praz': 1, '6h': 1, 'n': 2, 'aconselh': 1, 'ningu': 1, 'ussar': 1, 'atualiz': 3, 'celul': 2, 'comput': 1, 'cep': 2, 'comec': 1, 'zer': 2, 'sistem': 1, 'aceit': 1, 'apag': 1, 'edit': 2, 'br': 2, 'link': 1, 'quebr': 1, 'segur': 4, 'vari': 1, 'equip': 1, 'abre': 1, 'continu': 2, 'afirm': 1, 'enem': 1, 'dest': 1, 'ano': 1, 'incompetenc': 1, 'dess': 2, 'desastr': 1, 'govern': 1, 'mund': 2, 'mudanc': 2, 'migr': 1, 'outr': 3, 'avis': 1, 'facilitaca': 1, 'ocorr': 1, 'usad': 1, 'abrir': 1, 'part': 1, 'ai': 1, 'alter': 1, 'realiz': 1, 'receb': 1, 'mensag': 1, 'ap': 1, 'esquec': 1, 'mand': 1, 'codic': 1, 'verificaca': 2, 'sai': 1, 'ver': 1, 'qd': 1, 'inici': 1, 'favor': 1, 'arrum': 1, 'recom': 1, 'fcilidad': 1, 'uso': 1, 'palhac': 1, 'vo': 1, 'test': 2, 'atrav': 1, 'recadastr': 1, 'sup': 2, 'insatisfeit': 1, 'porq': 1, 'qr': 2, 'cod': 3, 'fod': 1, 'serv': 2, 'estress': 1, 'cidada': 1, 'tecnolog': 1, 'comport': 1, 'jeit': 2, 'not': 1, 'avali': 1, 'feliz': 1, 'mbbnb': 1, 'util': 4, 'fak': 1, 'otimiz': 1, 'ferrament': 2, 'exempl': 1, 'cnis': 1, 'auxili': 1, 'emergencial': 1, 'neg': 1, 'tant': 1, 'reclam': 1, 'realment': 2, 'ne': 1, '[?]': 1, 'tou': 1, 'poss': 1, 'verific': 1, 'imediat': 1, 'respost': 1, 'hilari': 1, 'pitoresc': 1, 'estapafurd': 1, 'diversa': 1, 'garant': 1, 'rir': 1, 'divert': 1, 'lembr': 1, 'grav': 1, 'compartilh': 1, 'alegr': 1, 'curt': 2, 'especial': 1, 'prest': 1, 'ignor': 1, 'opca': 1, 'necess': 1, 'pois': 1, 'dar': 1, 'entrad': 2, 'desempreg': 3, 'apric': 1, 'previst': 1, 'dezembr': 1, 'send': 1, 'desesper': 1, 'dand': 1, 'divergenc': 1, 'presencial': 1, 'cidad': 1, 'diss': 1, 'sint': 1, 'sozinh': 1, 'situaca': 2, 'advog': 1, 'complicaca': 1, 'falt': 1, 'abas': 1, 'informaca': 1, 'patif': 1, 'traz': 1, 'informaco': 1, 'incorret': 1, 'sus': 2, 'carta': 1, 'permit': 1, 'corrig': 1, 'bastant': 1, 'apes': 1, 'belez': 1, 'adiant': 1, 'princip': 1, 'pagin': 1, 'estagn': 1, 'formulari': 1, 'obrig': 1, 'agil': 1, 'ha': 1, 'carreg': 1}\n"
     ]
    }
   ],
   "source": [
    "print(corpus['not_usability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_low_score(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'form': 10,\n",
       " 'celul': 10,\n",
       " 'gov': 10,\n",
       " 'pois': 10,\n",
       " 'nenhum': 11,\n",
       " 'horrivel': 11,\n",
       " 'vez': 11,\n",
       " 'agor': 11,\n",
       " 'consegu': 12,\n",
       " 'email': 12,\n",
       " 'pod': 13,\n",
       " 'ajud': 13,\n",
       " 'opca': 13,\n",
       " 'ped': 13,\n",
       " 'entrar': 14,\n",
       " 'fiz': 14,\n",
       " 'fot': 14,\n",
       " 'ser': 15,\n",
       " 'aparec': 15,\n",
       " 'prov': 15,\n",
       " 'fac': 15,\n",
       " 'diz': 16,\n",
       " 'temp': 17,\n",
       " 'tod': 17,\n",
       " 'cnh': 17,\n",
       " 'nad': 18,\n",
       " 'vid': 19,\n",
       " 'fic': 19,\n",
       " 'cont': 20,\n",
       " 'nunc': 20,\n",
       " 'pessim': 21,\n",
       " 'cpf': 21,\n",
       " 'recuper': 22,\n",
       " 'precis': 25,\n",
       " 'acess': 26,\n",
       " 'reconhec': 27,\n",
       " 'erro': 28,\n",
       " 'aplic': 29,\n",
       " 'funcion': 29,\n",
       " 'facial': 30,\n",
       " 'ja': 32,\n",
       " 'app': 35,\n",
       " 'tent': 37,\n",
       " 'pra': 37,\n",
       " 'cadastr': 41,\n",
       " 'consig': 43,\n",
       " 'faz': 56,\n",
       " 'senh': 57,\n",
       " 'nao': 183}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(corpus['usability'].items(), key=lambda item: item[1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_corpus = {\n",
    "     'form': 10,\n",
    "     'celul': 10,\n",
    "     'vez': 11,\n",
    "     'consegu': 12,\n",
    "     'email': 12,\n",
    "     'pod': 13,\n",
    "     'ajud': 13,\n",
    "     'opca': 13,\n",
    "     'ped': 13,\n",
    "     'entrar': 14,\n",
    "     'fiz': 14,\n",
    "     'fot': 14,\n",
    "     'ser': 15,\n",
    "     'aparec': 15,\n",
    "     'prov': 15,\n",
    "     'fac': 15,\n",
    "     'diz': 16,\n",
    "     'temp': 17,\n",
    "     'tod': 17,\n",
    "     'cnh': 17,\n",
    "     'nad': 18,\n",
    "     'vid': 19,\n",
    "     'fic': 19,\n",
    "     'cont': 20,\n",
    "     'cpf': 21,\n",
    "     'recuper': 22,\n",
    "     'precis': 25,\n",
    "     'acess': 26,\n",
    "     'reconhec': 27,\n",
    "     'erro': 28,\n",
    "     'aplic': 29,\n",
    "     'funcion': 29,\n",
    "     'facial': 30,\n",
    "     'app': 35,\n",
    "     'tent': 37,\n",
    "     'cadastr': 41,\n",
    "     'consig': 43,\n",
    "     'faz': 56,\n",
    "     'senh': 57,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pois': 10,\n",
       " 'agor': 11,\n",
       " 'gov': 10,\n",
       " 'pessim': 21,\n",
       " 'nunc': 20,\n",
       " 'pra': 37,\n",
       " 'nenhum': 11,\n",
       " 'nao': 183,\n",
       " 'horrivel': 11,\n",
       " 'ja': 32}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla1 = {'form': 10,\n",
    " 'celul': 10,\n",
    " 'gov': 10,\n",
    " 'pois': 10,\n",
    " 'nenhum': 11,\n",
    " 'horrivel': 11,\n",
    " 'vez': 11,\n",
    " 'agor': 11,\n",
    " 'consegu': 12,\n",
    " 'email': 12,\n",
    " 'pod': 13,\n",
    " 'ajud': 13,\n",
    " 'opca': 13,\n",
    " 'ped': 13,\n",
    " 'entrar': 14,\n",
    " 'fiz': 14,\n",
    " 'fot': 14,\n",
    " 'ser': 15,\n",
    " 'aparec': 15,\n",
    " 'prov': 15,\n",
    " 'fac': 15,\n",
    " 'diz': 16,\n",
    " 'temp': 17,\n",
    " 'tod': 17,\n",
    " 'cnh': 17,\n",
    " 'nad': 18,\n",
    " 'vid': 19,\n",
    " 'fic': 19,\n",
    " 'cont': 20,\n",
    " 'nunc': 20,\n",
    " 'pessim': 21,\n",
    " 'cpf': 21,\n",
    " 'recuper': 22,\n",
    " 'precis': 25,\n",
    " 'acess': 26,\n",
    " 'reconhec': 27,\n",
    " 'erro': 28,\n",
    " 'aplic': 29,\n",
    " 'funcion': 29,\n",
    " 'facial': 30,\n",
    " 'ja': 32,\n",
    " 'app': 35,\n",
    " 'tent': 37,\n",
    " 'pra': 37,\n",
    " 'cadastr': 41,\n",
    " 'consig': 43,\n",
    " 'faz': 56,\n",
    " 'senh': 57,\n",
    " 'nao': 183}\n",
    "bla2 = tmp_corpus\n",
    "a = dict(set(bla1.items())-set(bla2.items()))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
